# beyond42

# Surveys
[pre-survey](https://docs.google.com/forms/d/e/1FAIpQLScuGwVxb0LW63_dBzsgqTPLXW-Mwx2sp6Y0dLW8DFBOOW1cbg/viewform?usp=sf_link)
[during-survey](https://docs.google.com/forms/d/e/1FAIpQLSfw3mmVwn8TP_0Z5rMpmesdRXBFMIcjtFvf8kA68i8o-6Dwfw/viewform?usp=sf_link)
[after-survey](https://docs.google.com/forms/d/e/1FAIpQLSevk8fYwRFWMyN0qFpssDXM-02lPkqeX3R2-fV_WjWgYQzsKw/viewform?usp=sf_link)

# Running
Run: `python query_llms.py` to pull the form responses and pass them to the LLMs (will need a service account credentials file with access to the form sheet)
Open: `templates/index.html` in a live server environment

# How can we trust an AI to navigate subjective questions?

Most of us hand over more and more responsibility to AI everyday. We compromise intent and decisions for convience and optimization. With this gained responsibility, AI is forced to enter the realm of subjective reasoning. 

To fully understand and embrace these new gained responsibilities, we should monitor not only the bias of the AI system we interact with but also the opinions. When we ask AI subjective questions we generally get hybrid responses addressing the wide variety of answers to these questions, but they lack definitive responses. By forcing the AI to choose between a set of human opinions, we can get a definitive answer from the AI system and therefore reveal its true opinions. Currently, the majority of human interactions with AIs are priotization tasks, such as social media feeds.

As AI systems increasingly mediate our experiences, they subtly influence our perceptions and decisions. For instance, recommendation algorithms on social media platforms determine which content we see, shaping our worldview without our explicit consent. This curation often reflects the biases inherent in the data used to train these algorithms, leading to echo chambers and reinforcing existing beliefs.

The challenge lies in the fact that AI lacks consciousness and moral reasoning. While it can process vast amounts of data and identify patterns beyond human capability, it doesn't possess the innate understanding of ethics or cultural nuances. This raises concerns about its ability to handle subjective matters fairly and empathetically.

To trust AI with subjective questions, transparency is crucial. Developers and organizations must disclose how AI systems make decisions, including the data sources and algorithms used. This openness allows for public scrutiny and the opportunity to address any embedded biases. Additionally, incorporating diverse data sets and perspectives can help mitigate partiality, ensuring the AI's outputs are more balanced.

Another approach is to keep humans in the loop. By involving human oversight in AI decision-making processes, especially in areas requiring ethical judgments, we can combine the efficiency of AI with human empathy and moral reasoning. This hybrid model leverages the strengths of both AI and human intellect.

Education also plays a vital role. Users should be informed about the limitations of AI and encouraged to question and critically evaluate the information presented to them. Promoting digital literacy helps individuals make more informed decisions rather than passively accepting AI-generated content.

In conclusion, while AI offers remarkable benefits in efficiency and problem-solving, trusting it with subjective questions necessitates caution. By emphasizing transparency, incorporating human oversight, and fostering critical engagement from users, we can better navigate the complexities of subjective reasoning in AI systems. Building this trust is essential for ensuring that AI serves as a tool that reflects and upholds the diverse values and ethics of the society it operates within.